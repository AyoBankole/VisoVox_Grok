{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fpdf\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import random\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from gtts import gTTS\n",
    "import base64\n",
    "import gdown\n",
    "from fpdf import FPDF\n",
    "import pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize OpenAI API \n",
    "# def get_env():\n",
    "#     api_key = os.getenv('OPENAI_API_KEY')\n",
    "#     if not api_key:\n",
    "#         raise ValueError(\"API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "#     return api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\"\n",
    "# !pip install gdown\n",
    "# Function to load the trained model, download it if necessary\n",
    "# Function to load the trained model, download it if necessary\n",
    "def load_trained_model():\n",
    "    model_path = \"model/blip_image_captioning_model.pth\"\n",
    "    drive_link = \"https://drive.google.com/drive/u/1/folders/1484tKLarTxDIIie_MkYWqh9Y7tbhiR-f\"\n",
    "\n",
    "    # Download the model if it does not exist\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        gdown.download(drive_link, model_path, quiet=False)\n",
    "\n",
    "    # Load the base model architecture from Hugging Face\n",
    "    base_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    # Load the custom state dict into the base model\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    base_model.load_state_dict(state_dict)\n",
    "    return base_model\n",
    "\n",
    "model = load_trained_model()\n",
    "\n",
    "def get_caption(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_to_speech(text, output_file):\n",
    "#     # OpenAI TTS request (assuming you have a TTS method)\n",
    "#     client = OpenAI()\n",
    "#     response = client.audio.speech.create(\n",
    "#     model=\"tts-1\",\n",
    "#     voice=\"alloy\",\n",
    "#     input=get_caption,\n",
    "# )\n",
    "\n",
    "#     audio_data = response['audio']\n",
    "#     with open(output_file, 'wb') as f:\n",
    "#         f.write(audio_data)\n",
    "\n",
    "# def save_model(model, path):\n",
    "#     torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    audio_fp = io.BytesIO()\n",
    "    tts.write_to_fp(audio_fp)\n",
    "    audio_fp.seek(0)\n",
    "    return audio_fp\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images from a folder\n",
    "def load_images(folder_path):\n",
    "    # List to store image file paths\n",
    "    image_files = []\n",
    "\n",
    "    # Supported image extensions\n",
    "    supported_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
    "\n",
    "    # Iterate over files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith(supported_extensions):\n",
    "            image_files.append(os.path.join(folder_path, file_name))\n",
    "\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected image: C:\\Users\\User\\OneDrive\\Desktop\\DATA SCIENCE\\GEN_ai\\VisoVox_Grok\\images\\antlers.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a group of deers\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Specify the folder path containing images\n",
    "    folder_path = r'C:\\Users\\User\\OneDrive\\Desktop\\DATA SCIENCE\\GEN_ai\\VisoVox_Grok\\images'\n",
    "    image_files = load_images(folder_path)\n",
    "    if not image_files:\n",
    "        print(\"No images found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    # Select a random image from the folder\n",
    "    random_image_path = random.choice(image_files)\n",
    "    print(f\"Selected image: {random_image_path}\")\n",
    "\n",
    "    # Get caption for the image\n",
    "    caption = get_caption(random_image_path)\n",
    "    if caption is None:\n",
    "        print(\"Could not generate a caption for the image.\")\n",
    "        return\n",
    "    print(\"Caption:\", caption)\n",
    "\n",
    "    # Convert caption to speech and get the audio file path\n",
    "    audio_fp = text_to_speech(caption)\n",
    "\n",
    "    # Initialize pygame mixer\n",
    "    pygame.mixer.init()\n",
    "\n",
    "    # Load and play the audio file\n",
    "    pygame.mixer.music.load(audio_fp, 'mp3')\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Wait for the audio to finish playing\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Call the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
